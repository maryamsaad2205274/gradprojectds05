import torch.optim as optim
from tqdm import tqdm
import math

criterion = nn.SmoothL1Loss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

def mean_pixel_error(pred_norm, gt_norm, widths, heights):
    B = pred_norm.shape[0]
    pred = pred_norm.view(B, 17, 2).clone()
    gt   = gt_norm.view(B, 17, 2).clone()

    w = torch.tensor(widths, device=pred.device).view(B,1,1)
    h = torch.tensor(heights, device=pred.device).view(B,1,1)
    pred_px = torch.stack([pred[:,:,0]*w.squeeze(-1), pred[:,:,1]*h.squeeze(-1)], dim=-1)
    gt_px   = torch.stack([gt[:,:,0]*w.squeeze(-1),   gt[:,:,1]*h.squeeze(-1)], dim=-1)

    d = torch.sqrt(((pred_px - gt_px)**2).sum(dim=-1))  # (B,17)
    return d.mean().item()

def nme(pred_norm, gt_norm, widths, heights):
    B = pred_norm.shape[0]
    diag = [math.sqrt(w*w + h*h) for w,h in zip(widths, heights)]
    mpe = mean_pixel_error(pred_norm, gt_norm, widths, heights)
    return mpe / (sum(diag)/len(diag))

def get_sizes_from_batch_indices(ds_subset):
    base_ds = ds_subset.dataset
    sizes = []
    for idx in ds_subset.indices:
        lbl_path = base_ds.label_files[idx]
        d = json.load(open(lbl_path))
        sizes.append((d["width"], d["height"]))
    return sizes

val_sizes = get_sizes_from_batch_indices(val_ds)
train_sizes = get_sizes_from_batch_indices(train_ds)

best_val_loss = 1e9
SAVE_PATH = os.path.join(BASE, "models_best_resnet18.pth")

for epoch in range(1, 61):  
    model.train()
    train_loss = 0.0

    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        pred = model(x)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * x.size(0)

    train_loss /= len(train_ds)

    model.eval()
    val_loss = 0.0
    all_pred, all_gt = [], []
    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.to(device)
            pred = model(x)
            loss = criterion(pred, y)
            val_loss += loss.item() * x.size(0)
            all_pred.append(pred.cpu())
            all_gt.append(y.cpu())

    val_loss /= len(val_ds)
    all_pred = torch.cat(all_pred, dim=0)
    all_gt = torch.cat(all_gt, dim=0)

    widths = [w for w,h in val_sizes]
    heights = [h for w,h in val_sizes]
    mpe = mean_pixel_error(all_pred, all_gt, widths, heights)
    val_nme = nme(all_pred, all_gt, widths, heights)

if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), SAVE_PATH)

    print(f"Epoch {epoch:03d} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | MPE {mpe:.2f}px | NME {val_nme:.4f} | best {best_val_loss:.4f}")
